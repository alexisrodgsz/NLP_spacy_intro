{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPShdfbov1IZRu59hBTGMmM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85yFLgjXNL9A"
      },
      "outputs": [],
      "source": [
        "## pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## pip install -U spacy"
      ],
      "metadata": {
        "id": "eRVOX_Hr0MRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## !pip install spacy"
      ],
      "metadata": {
        "id": "afEyvyYz6ngE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M65l_Sk13-8I",
        "outputId": "acc230ff-8646-423c-bac7-1f930034ece2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "metadata": {
        "id": "YLKvJNn95t-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3923ce6c-0be6-49a6-e18f-f13a8e8cc904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.7.4                         \n",
            "Location         /usr/local/lib/python3.10/dist-packages/spacy\n",
            "Platform         Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Python version   3.10.12                       \n",
            "Pipelines        en_core_web_md (3.7.1), en_core_web_sm (3.7.1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP Operations\n",
        "Processing Pipeline\n",
        "Tokenization - Lemmatization - Part-of-speech tagging - Syntactic dependency parsing - Named entity Recognition"
      ],
      "metadata": {
        "id": "JcKXQuOtb_Be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization\n"
      ],
      "metadata": {
        "id": "c_4P28ztenjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "id": "GtuhqYTs_hCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"I'm flying to Frisco\")\n",
        "print([w.text for w in doc])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o90sftVa_vQO",
        "outputId": "48bcfa67-98a1-468c-d710-e3ec0378dc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'m\", 'flying', 'to', 'Frisco']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemamatization\n"
      ],
      "metadata": {
        "id": "tYT8QjwQVlGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"this produc integrates both libraries for downloading and applying patches\")\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6jsgBZlInTd",
        "outputId": "67a7be65-251d-4610-cdfb-31f7996e1076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this this\n",
            "produc produc\n",
            "integrates integrate\n",
            "both both\n",
            "libraries library\n",
            "for for\n",
            "downloading download\n",
            "and and\n",
            "applying apply\n",
            "patches patch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization for Meaning Recognition:\n",
        "\n",
        "To determine this, the app searches for a word that matches one of the keywords in the predefined list. An easy way to simplify the search for these\n",
        "keywords is to first convert all the words in a sentence being processed to their lemmas. Other case is to add city nicknames .\n",
        "\n",
        "Define a special case"
      ],
      "metadata": {
        "id": "ZgRuU4u09cRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.symbols import ORTH, LEMMA, NORM\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "special_case = [{ORTH:'Frisco', NORM:'San Francisco'}]\n",
        "nlp.tokenizer.add_special_case('Frisco', special_case)\n",
        "## doc = nlp('I am flying to Frisco')\n",
        "print([w.text for w in nlp(\"I am flying to Frisco\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMjLBH9yUBf5",
        "outputId": "405f3933-2572-4cbf-d649-53d592b0b516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'flying', 'to', 'Frisco']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "nlp = spacy.load('en_core_web_md', cache_disabled=True)\n",
        "special_case = [{'ORTH': 'Frisco', 'NORM': 'San Francisco'}]\n",
        "nlp.tokenizer.add_special_case('Frisco', special_case)\n",
        "\n",
        "print([w.text for w in nlp(\"I am flying to Frisco\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "ksGN0Qew5CuZ",
        "outputId": "bb14e84f-60ff-4cf9-e1a1-00fcf41cd800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "load() got an unexpected keyword argument 'cache_disabled'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-5fc2c52596d5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import spacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_disabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mspecial_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Frisco'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NORM'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'San Francisco'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Frisco'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: load() got an unexpected keyword argument 'cache_disabled'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.symbols import ORTH, LEMMA, NORM\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "special_case = [{ORTH:'Frisco', LEMMA:'San Francisco'}]\n",
        "\n",
        "nlp.tokenizer.add_special_case('Frisco', special_case)\n",
        "doc = nlp('I am flying to Frisco')\n",
        "print([(token.text, token.lemma_) for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "zVEh5HsA5oz2",
        "outputId": "951b329e-c999-4aa6-fefe-e34b7c69aaa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Frisco'. Tokenizer exceptions are only allowed to specify ORTH and NORM.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-0f1d4c08c02c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspecial_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Frisco'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'San Francisco'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Frisco'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I am flying to Frisco'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._validate_special_case\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Frisco'. Tokenizer exceptions are only allowed to specify ORTH and NORM."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "# Load the spaCy language model\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "# Define and register a custom pipeline component\n",
        "@Language.component(\"custom_lemma\")\n",
        "def custom_lemma(doc):\n",
        "    for token in doc:\n",
        "        if token.text == 'Frisco':\n",
        "            token.lemma_ = 'San Francisco'\n",
        "    return doc\n",
        "\n",
        "# Add the custom component to the pipeline\n",
        "nlp.add_pipe(\"custom_lemma\", after='ner')\n",
        "\n",
        "# Process a text that includes the special case\n",
        "doc = nlp(\"I am flying to Frisco\")\n",
        "\n",
        "# Print the token text and lemmas\n",
        "print([(token.text, token.lemma_) for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK_UWSV977A-",
        "outputId": "3d41f3d9-8eab-405c-9ff2-15b4a080ce34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'I'), ('am', 'be'), ('flying', 'fly'), ('to', 'to'), ('Frisco', 'San Francisco')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## other\n",
        "\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "@Language.component(\"custom_lemma\")\n",
        "def custom_lemma(doc):\n",
        "  for token in doc:\n",
        "    if token.text == 'Frisco':\n",
        "      # Set lemma to the desired string\n",
        "      token.lemma = \"San Francisco\"  # String value for lemma\n",
        "  return doc\n",
        "\n",
        "# Load the spaCy language model\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "# Add the custom component to the pipeline\n",
        "nlp.add_pipe(\"custom_lemma\", after='ner')\n",
        "\n",
        "# Process a text that includes the special case\n",
        "doc = nlp(\"I am flying to Frisco\")\n",
        "\n",
        "# Print the token text and lemmas\n",
        "print([(token.text, token.lemma_) for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "JL5ULFt1Eu73",
        "outputId": "cb6361b9-70c6-4e23-ce39-ee63ecc33287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "an integer is required",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-55dc7791085b>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Process a text that includes the special case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I am flying to Frisco\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Print the token text and lemmas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE109\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0merror_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturned_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mraise_error\u001b[0;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-55dc7791085b>\u001b[0m in \u001b[0;36mcustom_lemma\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Frisco'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;31m# Set lemma to the desired string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"San Francisco\"\u001b[0m  \u001b[0;31m# String value for lemma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/token.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.lemma.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: an integer is required"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "def fix_frisco_lemma(doc: Doc) -> Doc:\n",
        "  \"\"\"\n",
        "  Post-processing function to modify lemma for \"Frisco\".\n",
        "  \"\"\"\n",
        "  for token in doc:\n",
        "      if token.text == \"Frisco\":\n",
        "          token.lemma_ = \"San Francisco\"  # Modify lemma_ after tokenization\n",
        "  return doc\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "doc = nlp(\"I am flying to Frisco\")\n",
        "doc = fix_frisco_lemma(doc)  # Apply post-processing function\n",
        "\n",
        "print([(token.text, token.lemma_) for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyxhfBt_FJtj",
        "outputId": "961ead8a-68f2-4d45-d1fa-cc8acb963426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'I'), ('am', 'be'), ('flying', 'fly'), ('to', 'to'), ('Frisco', 'San Francisco')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39TEAWIJEwao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importance of Pipeline Order\n",
        "In spaCy, the processing pipeline consists of various components that operate in a sequence. The order of these components can significantly impact the processing results because each component relies on the output of the previous one. Here are some common components in a spaCy pipeline:\n",
        "\n",
        "Tokenizer: Splits the raw text into individual tokens.\n",
        "\n",
        "Tagger: Assigns part-of-speech tags to tokens.\n",
        "\n",
        "Parser: Analyzes the syntactic structure of the sentence.\n",
        "\n",
        "NER (Named Entity Recognizer): Identifies named entities in the text.\n",
        "\n",
        "Custom Components: Any additional custom processing logic.\n",
        "\n",
        "Why Use after='ner'\n",
        "\n",
        "The ner component identifies named entities in the text and assigns them specific labels (such as \"ORG\" for organizations, \"PERSON\" for people, etc.). If your custom component needs to modify token attributes based on whether they are part of a named entity, it is essential to ensure that the NER component has already run.\n"
      ],
      "metadata": {
        "id": "TIyo2uRvsD2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of Speech Tagging\n",
        "\n",
        "Verbs: Tense, Aspect(simple, prgressive, or perfect), person and number\n",
        "\n",
        "Noun pronoun determiner, these are called coarse-grained parts of speech and are available as a fixed set of tags through the Token.pos (int) and Token.pos_ (unicode) attributes.\n",
        "\n",
        "Also, spaCy offers fine-grained parts of speech tags that provide more detailed information about a token.The finegrained part-of-speech tags are available as the Token.tag (int) and Token.tag_ (unicode) attributes.\n"
      ],
      "metadata": {
        "id": "x8rOMyq9ZTbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')\n",
        "doc = nlp('I have flown to LA. Now I am flying to Frisco')\n",
        "print([w.text for w in doc if w.tag_ == 'VBG'or w.tag_ == 'VB'])"
      ],
      "metadata": {
        "id": "NlXXEVa4ZTFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9aed8b7-3c7e-455f-90a4-e82a329fbd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['flying']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of Speech\n",
        "The tag_ property of a Token object contains the fine-grained part-of-speech attribute assigned to that object"
      ],
      "metadata": {
        "id": "9aqZFMjcvNf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('I have flown to LA. Now I am flying to Frisco')\n",
        "print([w.text for w in doc if w.pos_ == 'PROPN' ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf5ZE0EN4NfW",
        "outputId": "13546d2d-bf1f-422c-bbce-f42318ba1a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['LA', 'Frisco']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([w.text for w in doc if w.pos == spacy.symbols.PROPN])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZoitdxK0cDY",
        "outputId": "d89539d5-2e49-441e-b521-5dfc1af4f598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['LA', 'Frisco']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I am a runner\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.pos, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obxEtU3B2-z5",
        "outputId": "d8875fd8-9643-439b-d4c0-ea6644e6a5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I 95 PRON\n",
            "am 87 AUX\n",
            "a 90 DET\n",
            "runner 92 NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hb0Wz6IHYHsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context is important:\n",
        "\n",
        "the utterance might\n",
        "mean either “I'm already in the sky, flying to LA.” or “I'm\n",
        "going to fly to LA.” 91\n",
        "\n"
      ],
      "metadata": {
        "id": "ZL6khmfcaJw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntactic Relations\n",
        "\n",
        "Constituent-Based Structure  / Word-based sturucture\n",
        "\n",
        "The phrase structure tree breaks up the sentence based on the fact that the sentence consists of a noun phrase and a verb phrase.(second level hierarchy)\n",
        "\n",
        "Head: A word that governs or determines the properties of another word.\n",
        "Child (or Dependent): A word that depends on the head and is governed by it.\n",
        "Root: The topmost node in the tree, typically the main verb or predicate of the sentence.\n",
        "Dependency Relation: The type of syntactic relationship between a head and its child.\n"
      ],
      "metadata": {
        "id": "nJFmXl6sbCwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp =spacy.load('en_core_web_md')\n",
        "doc = nlp('I have flown to LA. Now I am flying to Frisco')\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_ , token.tag_ ,' ' , token.dep_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eza5c93vZ6yI",
        "outputId": "09327435-8dec-42cb-ed07-8b87cbd4e9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON PRP   nsubj\n",
            "have AUX VBP   aux\n",
            "flown VERB VBN   ROOT\n",
            "to ADP IN   prep\n",
            "LA PROPN NNP   pobj\n",
            ". PUNCT .   punct\n",
            "Now ADV RB   advmod\n",
            "I PRON PRP   nsubj\n",
            "am AUX VBP   aux\n",
            "flying VERB VBG   ROOT\n",
            "to ADP IN   prep\n",
            "Frisco PROPN NNP   pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "knPJA4yRJv0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.head.text, token.dep_ , token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDneSg4SZpY6",
        "outputId": "3d1987ed-65fd-4edd-adf1-ba5b88abfc24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flown nsubj I\n",
            "flown aux have\n",
            "flown ROOT flown\n",
            "flown prep to\n",
            "to pobj LA\n",
            "flown punct .\n",
            "flying advmod Now\n",
            "flying nsubj I\n",
            "flying aux am\n",
            "flying ROOT flying\n",
            "flying prep to\n",
            "to pobj Frisco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's try to figure out what labels point to the tokens that could potentially best describe the customer's intent. You need to find a pair that would alone appropriately describe the customer's intent.\n",
        "\n",
        "Interested in the tokens marked with the ROOT and pobj dependency labels, because in this example they're key in intent recognition. they marks the entity that—in conjunction with the verb— summarizes the meaning of the entire utterance."
      ],
      "metadata": {
        "id": "SatrnvK6jIud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentences level indices\n"
      ],
      "metadata": {
        "id": "Ay-eJBx7j5fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in doc.sents:\n",
        "  print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'pobj'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgRxPtueZ1iP",
        "outputId": "39c80840-1991-44ad-b5f0-4d248ec82ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['flown', 'LA']\n",
            "['flying', 'Frisco']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBOz7LqRknyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q6tgLWYhBLIs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}